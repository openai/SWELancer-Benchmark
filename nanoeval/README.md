# nanoeval

Simple, ergonomic, and high performance evals.

# Confidentiality note

`nanoeval` and its dependencies are shared with select Tier II contractors. As such, internal dependencies should be kept to a minimum as new dependencies require review; this is enforced via tests. `nanoeval_extra` contains monorepo-specific code.

# Principles

1. **Minimal indirection.** You should be able to implement and understand an eval in 100 lines.
2. **Separation of concerns.** Keep data loading away from completions/parsing/different ways of running an eval.
3. **Fast iteration and testability.** nanoevals should import in less than a second, and should be testable without an engine. No convoluted runners, remote rcall executions, etc. unless it's truly necessary.
4. **High performance**. Nanoeval should max out the compute resources available to it.

# Primitives

- `Eval` - A chz class. Enumerates a set of tasks, and (typically) uses a "Solver" to solve them and then records the results. Can be configured in code or on the CLI using a chz entrypoint.
- `EvalSpec` - An eval to run and runtime characteristics of how to run it (i.e. concurrency, recording, other administrivia)
- `Task` - A separable, scoreable unit of work.
- `Solver` - A strategy (usually involving sampling a model) to go from a task to a result that can be scored. For example, there may be different ways to prompt a model to answer a multiple-choice question (i.e. looking at logits, using consensus, etc)

# Running your first eval

See [gpqa_simple.py](nanoeval/examples/gpqa_simple.py) for an implementation of GPQA in <70 lines of code.

# Features

## Core execution flow

At the highest level: nanoeval is just a library. You can import it and call `nanoeval.run()` on an EvalSpec. nanoeval then loads all the tasks and runs `eval.evaluate()` in parallel using asyncio.

More granularly: nanoeval operates like a tiny distributed system. Eval state is tracked in a per-process sqlite database in `/dev/shm` (or `/tmp` on macOS). When you call `.run()`, it queues up the eval and all of its tasks in sqlite. It then starts one or more executors that continually poll the db for new tasks, run them, and put the results back in the db.

The executors can operate in two modes:

1. **In-process:** The executor is just an async task running in the same process as the main eval script. The default.
2. **Multiprocessing:** Starts a pool of executor processes that all poll the db. Use this via `spec.runner.experimental_use_multiprocessing=True`.

## How to accomplish common usage patterns ("eval a checkpoint", "eval a series of checkpoints")

1. Do it yourself by defining a script with `nanoeval_entrypoint()` that calls `nanoeval.run()`. See [nanoeval/examples/gpqa_simple.py](nanoeval/examples/gpqa_simple.py) for an example of this.
2. Eval an engine you already have running: `oaipkg run nanoeval_extra.checkpoints.eval_engine --help` 
3. Eval a checkpoint on a GPU devbox: `oaipkg run nanoeval_extra.checkpoints.eval_checkpoint --help`
4. Evaluate a series of checkpoints from a mini experiment: `oaipkg run nanoeval_extra.checkpoints.watcher --help`

## The monitor

Nanoeval has a tiny built-in monitor to track ongoing evals. It's a streamlit that visualizes the state of the internal run state database. This can be helpful to diagnose hangs on specific tasks. To use it:

```bash
# either set spec.runner.use_monitor=True OR run on devbox:
python3 -m nanoeval.bin.mon

# Then, ensure brix bastion is installed (see http://go/brix-bastion) and run:

brix open "$RCALL_JOB_NAME" --port 8501
```

## Resumption

Because nanoeval uses a persistent database to track the state of individual tasks in a run, this means you can restart an in-progress eval if it crashes. To do this:

```bash
python3 -m nanoeval.extras.resume db_name=<NAME OF YOUR RUN DB>
```

The `db_name` is typically autogenerated and looks something like `<computer hostname>-<pid>`. You can list all your databases with:

```bash
# For contractors
ls -lh ~/Library/Application Support/nanoeval/run_state/
# For openai employees
bbb ll az://oai$OPENAI_USER/nanoeval/run_state/
```

# **Writing your first eval**

An eval is just a `chz` class that defines `get_name()`, `get_tasks()`, `evaluate()` and `get_summary()`. Start with `gpqa_simple.py`; copy it and modify it to suit your needs. If necessary, drop down to the base `nanoeval.Eval` class instead of using `MCQEval`.

The following sections describe common use case needs and how to achieve them.

## **Public API**

You may import code from any `nanoeval.*` package that does not start with an underscore. Functions and classes that start with an underscore are considered private; if I refactor them, I will not make changes to your code (I'll # type: ignore it and ping you).

## Evallib compatibility

Nanoeval uses the same recorder as evallib, so you can use all the same tools (evalboard, etc.) for visualization. I am planning to make an evallib compat wrapper, so you can include nanoevals in evallib eval sets, but I haven't done that yet.

## Handling complex objects

In chz, adding simple attributes is simple; just add something like `max_tokens: int` in the class declaration. However, you may want to create more complex objects (Engines, TokenCompleters, etc.) which cannot be configured from the CLI. To do this, use a calculated property:

```python
import chz
from nanoeval import Eval
from token_completer import TokenCompleter
from functools import cached_property

@chz.chz
class MyEval(Eval):
    ...
    token_completer: TokenCompleter.Config

    @cached_property
    def token_completer_value(self) -> TokenCompleter:
        return self.token_completer.build()
```

I recommend using `@cached_property` if your property depends on state only available during `evaluate()` (e.g. the presence of an engine at `http://localhost:5001`), and `@chz.init_property` otherwise (e.g., if you're making a `tiktoken.Encoding`).

## Handling closable state

Many things you might want to use in an eval, like `TokenCompleter`s and `Agent`s, require one-time creation and cleanup. To accomodate this, `nanoeval.run` will `__aenter__` an eval on start and `__aexit__` on exit. We also have a simple wrapper for these functions called `HasAsyncContextManager` that lets you do something like this:

```python
from typing_extensions import override
from typing import AsyncGenerator
import chz
from nanoeval import Eval
from nanoeval.solvers.mcq import MCQTask, Answer, MCQSolver

@chz.chz
class MCQEval(Eval[MCQTask, Answer]):
    solver: MCQSolver
    n_consensus: int = 1

    @override
    async def _context(self) -> AsyncGenerator[None, None]:
        async with self.solver:
            yield
```

# Debugging

Is your big eval not working? Check here. Lots of these are general and will apply to non-nanoeval workloads as well.

## Error FAQ

- If you get errors like `boostedblob.request.RequestFailure: Reason: 'The specified container does not exist.'`, then you probably don't have a nanoeval container (`az://oai$OPENAI_USER/nanoeval`). You can check by running `bbb ls az://oai$OPENAI_USER`. If you don't see a `nanoeval` directory, then run `az storage container create --account-name oai$OPENAI_USER --name nanoeval --auth-mode login`.

## Killing old executors

Sometimes, if you ctrl-c the main job, executors don’t have time to exit. A quick fix:

```bash
pkill -f multiprocessing.spawn
```

## Observability

### py-spy/aiomonitor

`py-spy` is an excellent tool to figure out where processes are stuck if progress isn’t happening. You can check the monitor to find the PIDs of all the executors and py-spy them one by one. The executors also run `aiomonitor`, so you can connect to them via `python3 -m aiomonitor.cli ...` to inspect async tasks.